{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell: Imports and CUDA setup\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CUDA availability check\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU for training\")\n",
    "\n",
    "# Second cell: Dataset paths and functions\n",
    "# UPDATED PATH to dataset\n",
    "dataset_path = \"datasets/LSDSE-Dataset/data\"\n",
    "\n",
    "# UPDATED PATH for cross sections\n",
    "with open(\"datasets/LSDSE-Dataset/cross_sections.json\", \"r\") as f:\n",
    "    cross_sections = json.load(f)\n",
    "    print(f\"Loaded {len(cross_sections)} cross-section types\")\n",
    "\n",
    "# List all JSON files in dataset - UPDATED PATTERN to match data0.json to data39.json\n",
    "json_files = glob(os.path.join(dataset_path, \"data*.json\"))\n",
    "print(f\"Found {len(json_files)} building structure files\")\n",
    "\n",
    "# Function to convert LSDSE graph structure to PyTorch Geometric format\n",
    "def convert_to_pytorch_geometric(building_data):\n",
    "    \"\"\"\n",
    "    Convert an LSDSE building structure to PyTorch Geometric format.\n",
    "    \"\"\"\n",
    "    # Extract node features\n",
    "    # Format: [x1, y1, z1, x2, y2, z2, cross_section_one_hot, if_beam, if_boundary_beam, if_roof_beam, metal_deck_area]\n",
    "    \n",
    "    # Get bar node locations (endpoints)\n",
    "    bar_locations = np.array(building_data[\"bar_nodes\"][\"end_point_location\"])\n",
    "    \n",
    "    # Get one-hot cross section features\n",
    "    cross_sections_features = np.array(building_data[\"bar_nodes\"][\"cross_section\"])\n",
    "    \n",
    "    # Get other beam properties\n",
    "    if_beam = np.array(building_data[\"bar_nodes\"][\"if_beam\"]).reshape(-1, 1)\n",
    "    if_boundary_beam = np.array(building_data[\"bar_nodes\"][\"if_boundary_beam\"]).reshape(-1, 1)\n",
    "    if_roof_beam = np.array(building_data[\"bar_nodes\"][\"if_roof_beam\"]).reshape(-1, 1)\n",
    "    metal_deck_area = np.array(building_data[\"bar_nodes\"][\"metal_deck_area_on_beam\"]).reshape(-1, 1)\n",
    "    \n",
    "    # Combine all node features\n",
    "    node_features = np.hstack((\n",
    "        bar_locations,         # 6 features (x1,y1,z1,x2,y2,z2)\n",
    "        cross_sections_features, # 9 features (one-hot cross section)\n",
    "        if_beam,               # 1 feature\n",
    "        if_boundary_beam,      # 1 feature\n",
    "        if_roof_beam,          # 1 feature\n",
    "        metal_deck_area        # 1 feature\n",
    "    ))\n",
    "    \n",
    "    # Create edge index tensor from senders and receivers\n",
    "    edge_list = []\n",
    "    \n",
    "    # Column-column edges\n",
    "    if len(building_data[\"edges\"][\"column_column_senders\"]) > 0:\n",
    "        col_col_edges = np.stack([\n",
    "            building_data[\"edges\"][\"column_column_senders\"],\n",
    "            building_data[\"edges\"][\"column_column_receivers\"]\n",
    "        ], axis=0)\n",
    "        edge_list.append(col_col_edges)\n",
    "    \n",
    "    # Beam-column edges\n",
    "    if len(building_data[\"edges\"][\"beam_column_senders\"]) > 0:\n",
    "        beam_col_edges = np.stack([\n",
    "            building_data[\"edges\"][\"beam_column_senders\"],\n",
    "            building_data[\"edges\"][\"beam_column_receivers\"]\n",
    "        ], axis=0)\n",
    "        edge_list.append(beam_col_edges)\n",
    "    \n",
    "    # Column-ground edges\n",
    "    if len(building_data[\"edges\"][\"column_grounds_senders\"]) > 0:\n",
    "        col_ground_edges = np.stack([\n",
    "            building_data[\"edges\"][\"column_grounds_senders\"],\n",
    "            building_data[\"edges\"][\"column_grounds_receivers\"]\n",
    "        ], axis=0)\n",
    "        edge_list.append(col_ground_edges)\n",
    "    \n",
    "    # Combine all edge types\n",
    "    edge_index = np.concatenate(edge_list, axis=1) if edge_list else np.empty((2, 0), dtype=np.int64)\n",
    "    \n",
    "    # Extract drift ratios (target values for ML task)\n",
    "    drift_ratios = np.array(building_data[\"drift_ratio\"][:-1])  # Exclude ground node\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    y = torch.tensor(drift_ratios, dtype=torch.float)\n",
    "    \n",
    "    # Create PyTorch Geometric Data object\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        y=y,\n",
    "        num_nodes=len(node_features)\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Third cell: Memory-efficient dataset class\n",
    "class LSDSEDataset(Dataset):\n",
    "    def __init__(self, json_files, transform=None, pre_transform=None):\n",
    "        super(LSDSEDataset, self).__init__(None, transform, pre_transform)\n",
    "        self.json_files = json_files\n",
    "        \n",
    "        # Count total graphs (this reads all files once)\n",
    "        self.total_graphs = 0\n",
    "        for json_file in json_files:\n",
    "            with open(json_file, \"r\") as f:\n",
    "                self.total_graphs += len(json.load(f))\n",
    "        \n",
    "        # Create a mapping from index to (file_idx, structure_idx)\n",
    "        self.index_map = []\n",
    "        current_idx = 0\n",
    "        for file_idx, json_file in enumerate(json_files):\n",
    "            with open(json_file, \"r\") as f:\n",
    "                num_structures = len(json.load(f))\n",
    "                for struct_idx in range(num_structures):\n",
    "                    self.index_map.append((file_idx, struct_idx))\n",
    "                    current_idx += 1\n",
    "    \n",
    "    def len(self):\n",
    "        return self.total_graphs\n",
    "    \n",
    "    def get(self, idx):\n",
    "        file_idx, struct_idx = self.index_map[idx]\n",
    "        json_file = self.json_files[file_idx]\n",
    "        \n",
    "        with open(json_file, \"r\") as f:\n",
    "            building_structures = json.load(f)\n",
    "            building = building_structures[struct_idx]\n",
    "            return convert_to_pytorch_geometric(building)\n",
    "\n",
    "# Fourth cell: Data loading and visualization\n",
    "# REDUCED number of files to process initially for testing\n",
    "max_files_to_process = 2  # Start small, increase as you verify memory usage\n",
    "\n",
    "# Create the dataset\n",
    "dataset = LSDSEDataset(json_files[:max_files_to_process])\n",
    "print(f\"Dataset size: {len(dataset)} structures\")\n",
    "\n",
    "# Visualize a few examples\n",
    "for i in range(min(3, len(dataset))):\n",
    "    data = dataset[i]\n",
    "    \n",
    "    # Print information about the graph\n",
    "    print(f\"Graph {i}:\")\n",
    "    print(f\"  - Nodes: {data.num_nodes}\")\n",
    "    print(f\"  - Edges: {data.edge_index.size(1)}\")\n",
    "    print(f\"  - Features: {data.x.size(1)}\")\n",
    "    print(f\"  - Target shape: {data.y.shape}\")\n",
    "    \n",
    "    # Optional: Plot a simple visualization \n",
    "    # This assumes bar_nodes are arranged as [x1,y1,z1,x2,y2,z2,...] and uses just x,z coordinates\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Extract endpoints from features (first 6 columns)\n",
    "    for j in range(data.num_nodes):\n",
    "        x1, y1, z1, x2, y2, z2 = data.x[j, 0:6].cpu().numpy()\n",
    "        \n",
    "        # Use different colors for beams and columns (feature at index 15 is if_beam)\n",
    "        color = 'red' if data.x[j, 15] > 0.5 else 'blue'\n",
    "        \n",
    "        plt.plot([x1, x2], [z1, z2], color=color, linewidth=2)\n",
    "    \n",
    "    plt.title(f\"Building Structure - Example {i}\")\n",
    "    plt.xlabel(\"X-axis (ft)\")\n",
    "    plt.ylabel(\"Z-axis (ft)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Fifth cell: Create train/val/test split\n",
    "# 70/15/15 split\n",
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.3, random_state=42)\n",
    "val_indices, test_indices = train_test_split(test_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# Subset datasets\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders with appropriate batch size\n",
    "batch_size = 8  # REDUCED batch size for better memory management\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Sixth cell: Define GraphSAGE model\n",
    "class StructuralGraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
    "        super(StructuralGraphSAGE, self).__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Graph representation learning\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Final layer\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Seventh cell: Initialize model, optimizer and loss function\n",
    "# Get input dimension from data\n",
    "sample_data = dataset[0]\n",
    "in_channels = sample_data.x.size(1)  # Should be 19\n",
    "hidden_channels = 64\n",
    "out_channels = 4  # [Ex_x, Ex_y, Ey_x, Ey_y] drift ratios\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = StructuralGraphSAGE(in_channels, hidden_channels, out_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Eighth cell: Training and evaluation functions\n",
    "# Training function with CUDA support\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Target: drift ratios for each story\n",
    "        target = data.y.view(-1, out_channels)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "# Evaluation function with CUDA support\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            # Move data to device\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index)\n",
    "            target = data.y.view(-1, out_channels)\n",
    "            loss = criterion(out, target)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# Ninth cell: Train the model\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# ADDED checkpointing for longer training runs\n",
    "checkpoint_interval = 10  # Save every 10 epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_structural_model.pt\")\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Tenth cell: Evaluate on test set and visualize training results\n",
    "# Evaluate on test set\n",
    "test_loss = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Eleventh cell: Make predictions using trained model\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_structural_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Predict on a few test samples\n",
    "sample_idx = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        if i > 0:  # Just look at first batch\n",
    "            break\n",
    "            \n",
    "        data = data.to(device)\n",
    "        predictions = model(data.x, data.edge_index)\n",
    "        targets = data.y.view(-1, out_channels)\n",
    "        \n",
    "        # Convert back to numpy for easier analysis\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        targets = targets.cpu().numpy()\n",
    "        \n",
    "        # Show predictions vs targets for first few samples\n",
    "        for j in range(min(5, len(predictions))):\n",
    "            print(f\"Sample {sample_idx + j}:\")\n",
    "            print(f\"  Predicted drift ratios: {predictions[j]}\")\n",
    "            print(f\"  Actual drift ratios: {targets[j]}\")\n",
    "            print(f\"  Difference: {np.abs(predictions[j] - targets[j])}\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
